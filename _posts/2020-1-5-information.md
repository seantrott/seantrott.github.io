---
layout: post
title: Do different languages really convey information at the same rate?
---

Language is used across all known human populations, but the languages used across these populations also exhibit striking differences. A longstanding goal in Linguistics has thus been to identify **universal features** of language: characteristics that are shared across all human languages. Understanding which features appear to be *invariant* across languages would shed light on the origins of language, as well as the potential biological constraints that act as "filters" for the kinds of forms that languages can take.  

A recent paper ([Coupé et al, 2019](https://advances.sciencemag.org/content/5/9/eaaw2594?utm_source=TrendMD&utm_medium=cpc&utm_campaign=TrendMD_1)) argued for a kind of quasi-universal features: according to their analysis, languages convey "information" at roughly the same rate of 39 bits per second. This paper has since enjoyed a fair amount of discussion ([1](https://www.sciencedaily.com/releases/2019/09/190905124520.htm), [2](https://languagelog.ldc.upenn.edu/nll/?p=44386), [3](https://www.theatlantic.com/science/archive/2019/09/people-speak-faster-less-efficient-languages/597391/)). However, I think it's hard to understand or evaluate this claim without understanding how these variables (e.g., **information**) were operationalized. Thus, the goal of this post is to briefly describe the analyses performed in the paper.

# What is "information"?

The first and primary issue, in my view, is that the term "information" is used rather vaguely in everyday discourse, usually to mean something roughly like "semantic content". However, "information" also has a technical meaning, coming from the field of [information theory](https://en.wikipedia.org/wiki/Information_theory); this meaning is both more narrow and in some ways more complicated than the everyday meaning of "information", leading to considerable confusion.

[Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) introduced the technical meaning in a 1948 paper called [*A mathematical theory of communication*](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication) (Shannon, 1948). 




# References


Coupé, C., Oh, Y. M., Dediu, D., & Pellegrino, F. (2019). Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche. Science Advances, 5(9), eaaw2594.

Shannon, C. E. (1948). A mathematical theory of communication. Bell system technical journal, 27(3), 379-423.
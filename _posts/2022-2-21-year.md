---
layout: post
title: Some books I read in 2021 [other]
---

Last year I wrote up [mini-reviews of some books I read in 2020](https://seantrott.github.io/year/). I found (and continue to find) this post, and others like it, helpful primarily for the purpose of not forgetting books that felt particularly impactful when I read them. Sometimes there's a real pleasure in revisiting old thoughts and memories, and this kind of post serves as a useful cue to those memories. (There's probably something circular about writing a post to help remember the books that I find most memorable, or perhaps that's the point?)

The other purpose of a post like this is to inspire other people to read these books. Personally, I find that recommendations of any sort––for books, movies, songs, etc.––are much more meaningful when they come with an explanation for that recommendation. That's one of the things I find most lacking in many recommendation engines: "you may also like X" doesn't tell me *why* I may like X. So for each of these books, I'll try to emphasize what I found worth remembering.

[**The Alignment Problem**](https://brianchristian.org/the-alignment-problem/), by Brian Christian. This book helped convince me that research on long-term AI alignment is both important and under-resourced. I was already familiar with the many *current* examples of unethical uses of AI, e.g., as outlined in books like [Weapons of Math Destruction](https://www.penguinrandomhouse.com/books/241363/weapons-of-math-destruction-by-cathy-oneil/) or [Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/) (both of which I recommend). And I was also loosely familiar with a contingent of people, many of whom I associate with the [Rationalist movement](https://www.lesswrong.com/tag/rationalist-movement), who were concerned about *long-term*, *existential* threats of AI. But these long-term concerns always felt strangely under-specified and overly reliant on analogies, e.g., [a paperclip maximizer that decides to exterminate all humans](https://www.lesswrong.com/tag/paperclip-maximizer). I understood the underlying principles (I think), but I just didn't see a realistic scenario by which AI truly led to existential catastrophe––whereas it was much easier for me to see the existential risks posed by human-created pathogens or a world-ending comet. But as usual, it's rarely a good idea to dismiss intellectual ideas out of hand, especially when they're espoused by a community that one largely agrees with on other points. This book pushed me in the direction of taking long-term AI alignment more seriously. It's a broad survey of both current ethical issues in AI (including some of what O'Neil discusses in [Weapons of Math Destruction](https://www.penguinrandomhouse.com/books/241363/weapons-of-math-destruction-by-cathy-oneil/)]), as well as ongoing theoretical problems (like "black box" models) that pose serious practical issues for AI deployment. It's also just a really useful primer on major trends in AI, such as [inverse reinforcement learning](https://towardsdatascience.com/inverse-reinforcement-learning-6453b7cdc90d) and [deep neural networks](https://en.wikipedia.org/wiki/Deep_learning). Together with blog posts like [this one](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/), I'm considerably more convinced that work on AI safety and alignment is a valuable research program.


[**Golden Gates**](https://www.penguinrandomhouse.com/books/585765/golden-gates-by-conor-dougherty/), by Conor Dougherty. California has a massive [housing shortage](https://en.wikipedia.org/wiki/California_housing_shortage). (Side note: that Wikipedia page on the housing shortage is also quite informative.) The book is an in-depth look at the history and causes of the housing shortage in the California Bay Area, covering topics like [single-family zoning](https://en.wikipedia.org/wiki/Single-family_zoning), [neighborhood vetocracies](https://en.wikipedia.org/wiki/NIMBY), and more. It focuses on the rise of the [California YIMBYs](https://en.wikipedia.org/wiki/YIMBY), a political movement explicitly created to push back against NIMBYism and support housing development. I didn't know much about the history of housing regulations or zoning practices, so this book was a great introduction. I'm now convinced that housing is one of the most pressing problems facing California (and the USA more generally); I've also been convinced of the basic tenets of the YIMBY movement (e.g., upzoning, removing parking minimums, etc.). For a great, nuanced look at housing (and related) issues in the Bay Area, I recommend [Darrell Owens's Substack](https://darrellowens.substack.com/). [This article by Jerusalem Demas](https://www.vox.com/videos/2021/8/17/22628750/how-the-us-made-affordable-homes-illegal) is also really informative about how a variety of regulatory burdens have slowed the construction of new housing units and greatly contributed to our diminished housing stock.

[**Under a White Sky**](https://www.penguinrandomhouse.com/books/617060/under-a-white-sky-by-elizabeth-kolbert/), by Elizabeth Kolbert. Human civilization has wreaked havoc on the planet: there's [climate change](https://en.wikipedia.org/wiki/Climate_change), obviously, but there's also [biodiversity loss](https://en.wikipedia.org/wiki/Biodiversity_loss), [massive land erosion](https://en.wikipedia.org/wiki/Coastal_erosion_in_Louisiana), and more. And of course all these things are related. Kolbert describes a number of ways in which human existence has fundamentally altered the world in which we live––essentially a hard look at **unintended consequences**. And unfortunately, addressing these challenges likely requires *new* technological solutions and interventions in the environment. These solutions are bound to be controversial: there's the idea to [spray sulfur aerosols in the stratosphere](https://en.wikipedia.org/wiki/Stratospheric_aerosol_injection) to reduce the effects of warming; there's the idea of [genetically engineering invasive cane toads](https://theconversation.com/weve-cracked-the-cane-toad-genome-and-that-could-help-put-the-brakes-on-its-invasion-103362) to stop their spread; and then there's the idea of [sucking carbon dioxide from the air and injection it deep into stone under the ground](https://www.theguardian.com/environment/2021/sep/09/worlds-biggest-plant-to-turn-carbon-dioxide-into-rock-opens-in-iceland-orca). At first glance, many of these technological solutions feel like they're missing the point: if human intervention is what caused so many problems, why should we think further interventions will be any different? And the thing is that this question is absolutely valid––we *can't* know that these solutions won't introduce other unintended problems. In fact, it seems likely that they will. And that's what makes this reality so unsatisfying: the view of nature "untouched" by human intervention simply isn't tenable anymore––and crucially, **inaction is a form of action**. We've got to weigh the various *possible* paths ahead of us and figure out the least bad solution to the problems we're facing: and in many cases, technological wizardry is likely our best hope.


[**The Hole**](https://en.wikipedia.org/wiki/The_Hole_(novel)) by Hiroko Oyamada. 

What we talk about when we talk about love.


Frankenstein